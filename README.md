Enhanced Linear and Vision Transformer-Based Architectures for Time Series Forecasting

Abstract
Time series forecasting has been a challenging area in the field of Artificial Intelligence. Various approaches such as linear neural networks, recurrent linear neural networks, Convolutional Neural Networks, and recently transformers have been attempted for the time series forecasting domain. Although transformer-based architectures have been outstanding in the Natural Language Processing domain, especially in autoregressive language modeling, the initial attempts to use transformers in the time series arena have met mixed success. A recent important work indicating simple linear networks outperform transformer-based designs. We investigate this paradox in detail comparing the linear neural network- and transformer-based designs, providing insights into why a certain approach may be better for a particular type of problem. We also improve upon the recently proposed simple linear neural network-based architecture by using dual pipelines with batch normalization and reversible instance normalization. Our enhanced architecture outperforms all existing architectures for time series forecasting on a majority of the popular benchmarks.

https://www.mdpi.com/2504-2289/8/5/48


![Screenshot from 2024-06-09 18-06-05](https://github.com/muslehal/Enhanced-Linear-Model-ELM-/assets/111980129/fb11f357-1360-4424-8792-08e529a8acba)
